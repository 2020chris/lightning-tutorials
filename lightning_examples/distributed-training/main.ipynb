{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123e7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc4e30",
   "metadata": {},
   "source": [
    "# Distributed Training with PyTorch Lightning\n",
    "\n",
    "This tutorial covers several accelerator choices for multi-GPU distributed training and will walk you through the differences between these, when to use one over the other, and best practices in writing accelerator-agnostic code.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. [Multi-GPU Acceleration in Lightning](#multi-gpu)\n",
    "2. [Prerequisites](#prerequisites)\n",
    "\n",
    "3. [DDP: Distributed Data-Parallel](#ddp)\n",
    "4. [DDP-spawn: Distributed Data-Parallel Spawn](#ddp_spawn)\n",
    "5. [DP: Data-Parallel](#dp)\n",
    "6. [SDP: Sharded Data-Parallel](#sdp)\n",
    "7. [FSDP: Fully Sharded Data-Parallel](#fsdp) (COMING SOON)\n",
    "8. [Distributed Inference](#inference)\n",
    "9. [Best practices](#best-practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd162b",
   "metadata": {},
   "source": [
    "<a id='multi-gpu'></a>\n",
    "\n",
    "## Multi-GPU Acceleration in Lightning\n",
    "\n",
    "Lightning supports a variety of different accelerators and plugins for multi-GPU/distributed training. The **Accelerator** determines the hardware type we are running on. This can be a CPU, GPU, TPU or IPU. Part of the accelerator is also a **Plugin** (also referred to as \"training type plugin\", \"backend\" or \"distributed backend\" sometimes) that determines how model and data are split across multiple devices and it defines the communication and synchronization between devices and processes.\n",
    "\n",
    "This tutorial will focus on the **GPU accelerator** because it is compatible with a large selection of different plugins.\n",
    "Below we list all of the major choices that Lightning offers, each with recommendations when to use and when not to use, an example code, and important details to consider for writing device-agnostic and performant code.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96234",
   "metadata": {},
   "source": [
    "<a id='prerequisites'></a>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "In order to run multi-GPU experiments, you will need\n",
    "\n",
    "- A server or desktop machine with GPU devices\n",
    "- PyTorch installed with GPU support\n",
    "\n",
    "\n",
    "Throughout the next sections, we will re-use the following templates for the model and data module.\n",
    "\n",
    "**IMPORTANT NOTE:** This notebook is not meant to be executed in full. Some cells will produce an output but most of the backends presented here will NOT run in a Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1da403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "\n",
    "\n",
    "class MNISTDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir: str = \"./\", batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(), transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ])\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # only downloads the data once\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "    \n",
    "    \n",
    "class TutorialModule(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 128,\n",
    "        learning_rate: float = 0.0001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\n",
    "        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\n",
    "        self.val_accuracy = Accuracy(num_classes=10)\n",
    "        self.test_accuracy = Accuracy(num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return {\"loss\": loss, \"y_hat\": y_hat.detach()}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        self.log(\"val_acc\", self.val_accuracy(pred, y), prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        self.log(\"test_acc\", self.test_accuracy(pred, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc93cd7",
   "metadata": {},
   "source": [
    "We perform a quick test run to check that the template code works. It should achieve a test accuracy of ~92% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41173666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | l1            | Linear   | 100 K \n",
      "1 | l2            | Linear   | 1.3 K \n",
      "2 | val_accuracy  | Accuracy | 0     \n",
      "3 | test_accuracy | Accuracy | 0     \n",
      "-------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b16ac4efd8d4f709b00da9f59ef9aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecb4b09ae1147b8b7b11f1dcfa98349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9265999794006348}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.9265999794006348}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "trainer = Trainer(max_epochs=1)\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "trainer.test(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796e5c2",
   "metadata": {},
   "source": [
    "<a id='ddp'></a>\n",
    "\n",
    "## DDP: Distributed Data-Parallel\n",
    "\n",
    "**Use when:** \n",
    "- you want to scale your training to as many GPUs as you want;\n",
    "- you want to perform multi-node training.\n",
    "\n",
    "**Do not use when:**\n",
    "- you are running inside a Jupyter noteboook.\n",
    "\n",
    "**How to activate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db317b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    gpus=2, \n",
    "    accelerator=\"ddp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d7e35",
   "metadata": {},
   "source": [
    "The Distributed Data-Parallel (DDP) plugin in Lightning is orchestrating training in several processes. There are as many processes as devices are involved and these processes can either be launched directly by Lightning (default) or an external launch utilitiy like ``torch.distributed.launch``. The DDP plugin is the recommended choice by Lightning because it scales linearly (with a constant overhead) in the number of GPUs and has very few limitations for the average use case.\n",
    "\n",
    "**IMPORTANT:** DDP only works in script-mode, i.e., you need to be able to launch your program like so:\n",
    "```bash\n",
    "python train.py [ARGS]\n",
    "```\n",
    "It will NOT work in Jupyter notebooks, Google Colab, Kaggle, etc.\n",
    "\n",
    "Under the hood, Lightning calls the script (itself) several times to launch more processes, like so:\n",
    "\n",
    "```bash\n",
    "# this is what the user launches\n",
    "python train.py --gpus 4\n",
    "\n",
    "# lightning launches the same program an additional 3 times:\n",
    "LOCAL_RANK=1 python train.py --gpus 4\n",
    "LOCAL_RANK=2 python train.py --gpus 4\n",
    "LOCAL_RANK=3 python train.py --gpus 4\n",
    "```\n",
    "\n",
    "The local rank is what uniquely identifies each process. These processes will run independently in parallel and synchronize at certain points of their execution (more about that later). There are two important aspects crucial to the understanding of DDP; the model and the data.\n",
    "\n",
    "**Data:** The data gets partinioned into N subsets where N is the number of GPUs/processes. Each process only has access to its assigned subset and this is why the plugin is called data-parallel. The splitting of the data into each process is automatically taken care of by Lightning and the PyTorch distributed sampler.\n",
    "\n",
    "**Model:** The model, at any point in time, has the same parameter values across all GPUs. The difference between the processes are the gradients, because they get computed from data that is different in each process. Before a model is updated, the gradients are synchronized (averaged) so that after the optimizer update the model weights are all the same again across the processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4416eae",
   "metadata": {},
   "source": [
    "**Example 1:** No code changes are required to run with DDP. Simply set the Trainer argument for the accelerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f79dd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "trainer = Trainer(\n",
    "    gpus=4,\n",
    "    accelerator=\"ddp\",\n",
    ")\n",
    "\n",
    "# ATTENTION: only run this outside the Jupyter notebook:\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11fbb2",
   "metadata": {},
   "source": [
    "**Example 2:** If there is data to download, extract and preprocess before training, it is important to do this only once and only in one process. Otherwise, each process will write to the same files at the same time. In Lightning, we split this logic into two separate hooks: ``prepare_data()`` and ``setup()``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8751c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessExampleDataModule(MNISTDataModule):\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # runs only once and only in the process 0\n",
    "        # this hook is also available in the LightningModule\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # runs in each process\n",
    "        # this hook is also available in the LightningModule\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0907068",
   "metadata": {},
   "source": [
    "**Example 3**: With DDP it is also possible to run multi-node training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3b36ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "trainer = Trainer(\n",
    "    gpus=4,\n",
    "    num_nodes=2,\n",
    "    accelerator=\"ddp\",\n",
    "    # set to False if you are preparing data on a shared filesystem\n",
    "    # default is True\n",
    "    prepare_data_per_node=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180b980",
   "metadata": {},
   "source": [
    "Notice the ``prepare_data_per_node`` Trainer argument. The setting of this boolean depends on what we are doing in the ``prepare_data`` hook: In our example here we are downloading data, and we don't want to do that on every node if the filesystem is a shared across the servers.\n",
    "\n",
    "In the normal case, no other changes to the script are required to make multi-node training possible. However, the way the script gets launched depends on your cluster. Instructions how do so can be found in the [Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc697f37",
   "metadata": {},
   "source": [
    "**Example 4:** As mentioned before, the dataset gets partitioned and evenly distributed across the processes. This is possible with a [distributed sampler](https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler) which Lightning automatically adds for us. There are two important caveats we need to be aware of!\n",
    "\n",
    "1. If the dataset is not evenly divisible by the number of GPUs, then the distributed sampler will append enough \"fake\" samples such that all GPUs see the same number of samples. These samples are fake in the sense that they are copies of existing data and thus data distribution is slightly biased. It is necessary due to the way PyTorch synchronizes the processes and cannot be avoided.\n",
    "\n",
    "2. The `training_epoch_end`, `validation_epoch_end` and `test_epoch_end` hooks will receive ONLY the outputs of the step method in the respective process, NOT all outputs from all processes/GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9ae6826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPDataDemoModule(TutorialModule):\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        process_id = self.global_rank\n",
    "        print(f\"{process_id=} saw {len(outputs)} samples total\")\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f\"training set contains {len(self.trainer.datamodule.mnist_train)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "model = DDPDataDemoModule()\n",
    "datamodule = MNISTDataModule(batch_size=1)\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=4,\n",
    "    accelerator=\"ddp\",\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "# ATTENTION: only run this outside the Jupyter notebook:\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c895c28",
   "metadata": {},
   "source": [
    "In the terminal output we see the following:\n",
    "    \n",
    "```\n",
    "process_id=0 saw 13750 samples total\n",
    "process_id=1 saw 13750 samples total\n",
    "process_id=2 saw 13750 samples total\n",
    "process_id=3 saw 13750 samples total\n",
    "\n",
    "training set contains 55000 samples\n",
    "```\n",
    "As we can see, each GPU gets 55000 / 4 = 137500 data samples. However, what happens if we set the number of GPUs to 3, which does not divide 55000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e6548bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    # NOTICE: does not divide dataset size evenly\n",
    "    gpus=3,\n",
    "    accelerator=\"ddp\",\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "# ATTENTION: only run this outside the Jupyter notebook:\n",
    "# trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1ac7b",
   "metadata": {},
   "source": [
    "```\n",
    "process_id=0 saw 18334 samples total\n",
    "process_id=1 saw 18334 samples total\n",
    "process_id=2 saw 18334 samples total\n",
    "training set contains 55000 samples\n",
    "```\n",
    "\n",
    "Notice that we saw 18334 samples in each process, and 18334 * 3 = 55002, while 55000 % 3 = 1. This means the distributed sampler produced **one extra sample** in each process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7801eb7",
   "metadata": {},
   "source": [
    "**Example 5:** As we have seen in the previous example, the ``*_epoch_end`` hooks collect only the outputs for the current process. What if we want all outputs? It can be achieved by calling the ``LightningModule.all_gather`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d834f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPAllGatherDemoModule(TutorialModule):\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        return pred\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        process_id = self.global_rank\n",
    "        preds = torch.cat(outputs)\n",
    "        print(f\"{process_id=} saw {len(outputs)} test_step outputs, made {len(preds)} predictions\")\n",
    "\n",
    "        # gather all predictions into all processes\n",
    "        all_preds = self.all_gather(preds)\n",
    "        print(f\"{process_id=} all-gathered {all_preds.shape[0]} x {all_preds.shape[1]} predictions\")\n",
    "\n",
    "        # do something will all outputs\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f317708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "model = DDPAllGatherDemoModule()\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=4,\n",
    "    accelerator=\"ddp\",\n",
    "    max_epochs=1,\n",
    ")\n",
    "    \n",
    "# ATTENTION: only run this outside the Jupyter notebook:\n",
    "# trainer.test(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c77c0f",
   "metadata": {},
   "source": [
    "The output is:\n",
    "\n",
    "\n",
    "```\n",
    "process_id=0 saw 157 test_step outputs, made 2500 predictions\n",
    "process_id=1 saw 157 test_step outputs, made 2500 predictions\n",
    "process_id=2 saw 157 test_step outputs, made 2500 predictions\n",
    "process_id=3 saw 157 test_step outputs, made 2500 predictions\n",
    "\n",
    "process_id=0 all-gathered 4 x 2500 predictions\n",
    "process_id=1 all-gathered 4 x 2500 predictions\n",
    "process_id=2 all-gathered 4 x 2500 predictions\n",
    "process_id=3 all-gathered 4 x 2500 predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bd972",
   "metadata": {},
   "source": [
    "**Example 6:** Effective batch size, learning rate, number of workers. \n",
    "\n",
    "The batching happens independently in each process, i.e., the ``batch_size`` argument set in the data loader is local to the current process. The effective batch size can be computed like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7868ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.world_size=4\n",
      "datamodule.batch_size=16\n",
      "effective_batch_size=64\n"
     ]
    }
   ],
   "source": [
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "trainer = Trainer(gpus=4, accelerator=\"ddp\")\n",
    "\n",
    "effective_batch_size = datamodule.batch_size * trainer.world_size\n",
    "\n",
    "print(f\"{trainer.world_size=}\")\n",
    "print(f\"{datamodule.batch_size=}\")\n",
    "print(f\"{effective_batch_size=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a3c48",
   "metadata": {},
   "source": [
    "If, for example, we double the number of GPUs, the effective batch size automatically doubles too. By the rule of thumb, it is also advised to double the learning rate by that same factor. We can do that easily by defining a *base learning rate* for a single GPU and then multiply by the total GPUs, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92c7b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer.world_size=4\n",
      "base_learning_rate=0.001\n",
      "world_learning_rate=0.004\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=4, accelerator=\"ddp\")\n",
    "\n",
    "base_learning_rate = 0.001\n",
    "world_learning_rate = base_learning_rate * trainer.world_size\n",
    "\n",
    "print(f\"{trainer.world_size=}\")\n",
    "print(f\"{base_learning_rate=}\")\n",
    "print(f\"{world_learning_rate=}\")\n",
    "\n",
    "model = TutorialModule(learning_rate=world_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea7708",
   "metadata": {},
   "source": [
    "Also, the number of workers ``num_workers`` as well as all other settings for the dataloaders applies *per process*.\n",
    "Best practice is to tune the learning rate, batch size and num workers using a single GPU to a good initial value, and then scale up to many GPUs as shown above with no code changes required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c0014",
   "metadata": {},
   "source": [
    "<a id='ddp_spawn'></a>\n",
    "\n",
    "## DDP-spawn: Distributed Data-Parallel Spawn\n",
    "\n",
    "\n",
    "**Use when:** \n",
    "- you want to get the benefits of DDP / distributed data-parallel, but\n",
    "- you want to run DDP inside a Jupyter notebook.\n",
    "\n",
    "**Do not use when:**\n",
    "- you need to run multi-node training;\n",
    "- you want the fastest multi-GPU training (e.g., use DDP instead);\n",
    "- one or several of your Python objects are not picklable.\n",
    "\n",
    "**How to activate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdf9084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    gpus=4, \n",
    "    accelerator=\"ddp_spawn\",  # this is already the default in Lightning!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60d17f",
   "metadata": {},
   "source": [
    "\n",
    "DDP-spawn is a variation of [DDP](#ddp) and is the default accelerator when the ``gpus`` Trainer argument is used. The two behave identically when training a model, however, the spawn version launches the distributed processes differently, namely using the [``torch.multiprocessing.spawn``](https://pytorch.org/docs/stable/multiprocessing.html?highlight=spawn#torch.multiprocessing.spawn) function. A call to ``trainer.fit()`` (or test/validate/predict) with N GPUs will do the following:\n",
    "\n",
    "1. The main forks N new processes in which the model will train. The main process will wait and DO NOTHING until all worker processes finish. This is different from [DDP](#ddp) where the main process launches N-1 subprocesses and then continues to participate for training.\n",
    "\n",
    "2. When forking the processes, all objects in the main process get pickled and sent to the worker processes. This includes the initial weights of the model and any other objects defined by the user. These objects need to be picklable!\n",
    "\n",
    "3. When the worker processes finish training, the execution continues in the main process where ``trainer.fit()`` (or test/validate/predict) ends. At the same time, the model weights get copied to the main process (which so far was waiting and never training anything). **IMPORTANT:** Only the model weights get copied back to the main process.\n",
    "\n",
    "4. The worker processes die off and execution continues in the main process.\n",
    "\n",
    "\n",
    "This method of forking processes has several disadvantages.\n",
    "\n",
    "- The forking is expensive, especially when many dataloader workers (``num_workers``) are involved.\n",
    "- Every object needs to be picklable.\n",
    "\n",
    "In light of these limitations, [DDP](#ddp) offers a much better user experience overall. However, DDP-spawn and [DP](#dp) are the only accelerators that work in a Jupyter notebook (Google Colab, Kaggle, etc.). \n",
    "\n",
    "**Examples:** All examples and instructions in the [DDP](#ddp) section apply for DDP-spawn as well! All you have to do is change ``accelerator=\"ddp\"`` to ``accelerator=\"ddp_spawn\"``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4db31b",
   "metadata": {},
   "source": [
    "<a id='dp'></a>\n",
    "\n",
    "## DP: Data-Parallel\n",
    "\n",
    "**Use when:** \n",
    "- you want to port an existing PyTorch model written with DataParallel and want to maintain 100% parity;\n",
    "- your optimization needs the full aggregated batch of outputs/losses from all GPUs;\n",
    "- you need to run multi-GPU in a Jupyter notebook cell, and cannot convert to a script;\n",
    "- none of the other backends presented here are suitable due to their hardware and runtime requirements.\n",
    "\n",
    "**Do not use when:**\n",
    "- you are looking for the most performant multi-GPU code;\n",
    "- you have custom batch structures that can not be converted to primitive containers like tuples, lists, dicts etc.;\n",
    "- you rely heavily on torchmetrics.\n",
    "\n",
    "**How to activate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cf5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# data-parallel with 2 GPUs\n",
    "trainer = Trainer(\n",
    "    gpus=2, \n",
    "    accelerator=\"dp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20647836",
   "metadata": {},
   "source": [
    "Data-Parallel initially moves all model parameters, buffers and data tensors to the root GPU. In Lighting, this is GPU 0. The following steps take place in _every_ training step:\n",
    "1. The model gets replicated to every device, i.e., parameters and buffers get copied from the root device to all other devices. \n",
    "2. The data batch that initially resides on GPU 0 gets split into N sub-batches along dimension 0 (batch dimension). Each GPU receives one of these batch splits and they are passed to the ``training_step`` hook.\n",
    "3. The output of ``training_step`` in each device will be transferred back to the root device and averaged.\n",
    "\n",
    "The fact that the module is replicated every forward and backward pass makes this the least efficient plugin for multi-GPU training. An additional caveat is that state changes on the module during ``training_step`` are lost, and this is a common source of bugs. It is also the reason why torchmetrics is not recommended together with this plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa437de",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77ed22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPModule(TutorialModule):\n",
    "    \n",
    "    # *_step() happens on the replica of the model (each GPU runs this)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # total batch size = 16, 2 GPUs -> each GPU sees batch of size 8\n",
    "        # the last batch may still be smaller, the dataset may not be evenly divisible by the batch size \n",
    "        assert x.shape[0] <= 8 \n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        return pred, y\n",
    "\n",
    "    def validation_step_end(self, outputs):\n",
    "        # torchmetrics do not support sync on the replica\n",
    "        # all torchmetric computations need to be performed in *_step_end()\n",
    "        # which happens on the root device\n",
    "        pred, y = outputs\n",
    "        self.log(\"val_acc\", self.val_accuracy(pred, y), prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26f01bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/core/datamodule.py:167: LightningDeprecationWarning: DataModule property `has_prepared_data` was deprecated in v1.4 and will be removed in v1.6.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | l1            | Linear   | 100 K \n",
      "1 | l2            | Linear   | 1.3 K \n",
      "2 | val_accuracy  | Accuracy | 0     \n",
      "3 | test_accuracy | Accuracy | 0     \n",
      "-------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 1\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f80d0a9b2ce4328bafc335ee72199d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "model = DPModule()\n",
    "\n",
    "datamodule = MNISTDataModule()\n",
    "trainer = Trainer(\n",
    "    gpus=2, \n",
    "    accelerator=\"dp\", \n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978189ff",
   "metadata": {},
   "source": [
    "**Example 2: Custom reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "300e3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPModule(TutorialModule):\n",
    "    \n",
    "    def training_step_end(self, outputs):\n",
    "        # outputs is a dict\n",
    "        # it is the result of merging all dicts returned by training_step() on each device\n",
    "        \n",
    "        # the loss from each GPU, 2 GPUs are used here\n",
    "        losses = outputs[\"loss\"]\n",
    "        assert losses.shape[0] == 2\n",
    "        \n",
    "        # each GPU returned 8 predictions\n",
    "        y_hats = outputs[\"y_hat\"]\n",
    "        assert y_hats.shape[0] == 2 * 8\n",
    "        \n",
    "        probs = F.softmax(y_hats, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        loss = torch.mean(losses)\n",
    "        return {\"loss\": loss, \"pred\": preds}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        # we can receive all outputs from all training steps and concatenate them\n",
    "        all_predictions = torch.cat([out[\"pred\"] for out in outputs])\n",
    "        print(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b724fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/core/datamodule.py:167: LightningDeprecationWarning: DataModule property `has_prepared_data` was deprecated in v1.4 and will be removed in v1.6.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18f5b3033774805b8f64dbdb283e1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 2, 2, 3, 5, 2, 7, 7, 5, 9, 2, 2, 1, 7, 2, 3, 2, 2, 2, 3, 5, 2, 7,\n",
      "        7, 5, 9, 2, 2, 1, 7, 2, 3, 2, 2, 2, 3, 5, 2, 7, 7, 5, 9, 2, 2, 1, 7, 2,\n",
      "        3, 2, 2, 2, 3, 5, 2, 7, 7, 5, 9, 2, 2, 1, 7, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = DPModule()\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=2, \n",
    "    accelerator=\"dp\", \n",
    "    max_steps=4,\n",
    "    limit_val_batches=0,\n",
    ")\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac161b",
   "metadata": {},
   "source": [
    "<a id='sdp'></a>\n",
    "\n",
    "\n",
    "## SDP: Sharded Data-Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113c0db",
   "metadata": {},
   "source": [
    "**Use when:** \n",
    "- memory is a concern because model parameters + optimizer + gradients do not fit on a GPU;\n",
    "- your model has >= 500 million parameters;\n",
    "- you are using very large batch sizes or inputs.\n",
    "\n",
    "**Do not use when:**\n",
    "- your model is small enough to fit on a single GPU.\n",
    "\n",
    "**How to activate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f3488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    gpus=4, \n",
    "    accelerator=\"ddp_sharded\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b4381",
   "metadata": {},
   "source": [
    "Sharded Data Parallel (SDP) offers significant memory savings for very large models (above 500M parameters). It enables one to train models that would normally not fit onto a single GPU, or allows for an increased batch- or input size.\n",
    "\n",
    "The motivation behind sharded training comes from the observation that gradients and optimizer state are usually dominating the memory during training. This is especially significant for optimizers such as Adam where per-parameter weights and running averages are kept in memory.\n",
    "\n",
    "**Example**: Memory comparison (artificial example). Here we want to draw a simple comparison between DDP and Sharded DDP using the MNIST toy example. Note, the MNIST classifier is way too small to see practical benefits here, but for demonstration purposes it will suffice to show minor memory efficiency.\n",
    "\n",
    "First we measure the memory footprint when training with ``accelerator=\"ddp\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1649835",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=2,\n",
    "    accelerator=\"ddp\",\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "# ATTENTION: only run this outside the Jupyter notebook:\n",
    "# trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "ddp_max_mem = torch.cuda.max_memory_allocated(trainer.local_rank) / 1000\n",
    "print(f\"GPU {trainer.local_rank} max memory using DDP: {ddp_max_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf4aa7",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "GPU 0 max memory using DDP: 2859.52 MB\n",
    "GPU 1 max memory using DDP: 2859.52 MB\n",
    "```\n",
    "\n",
    "Next, we switch to ``accelerator=\"ddp_sharded\"``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f57767",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=2,\n",
    "    accelerator=\"ddp_sharded\",\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "# ATTENTION: only run this outside the Jupyter notebook:\n",
    "# trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "sdp_max_mem = torch.cuda.max_memory_allocated(trainer.local_rank) / 1000\n",
    "print(f\"GPU {trainer.local_rank} max memory using SDP: {sdp_max_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68503c4",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "GPU 0 max memory using SDP: 2433.02 MB\n",
    "GPU 1 max memory using SDP: 905.22 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472258a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22d6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81c420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec3c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
