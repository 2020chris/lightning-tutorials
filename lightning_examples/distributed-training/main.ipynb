{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bc4e30",
   "metadata": {},
   "source": [
    "# Distributed Training with PyTorch Lightning\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "Overview of multi-device backends in Lightning\n",
    "\n",
    "1. DP: Data-Parallel\n",
    "2. DDP: Distributed Data-Parallel\n",
    "3. DDP-spawn: Distributed Data-Parallel Spawn\n",
    "4. DDP2: DP and DDP Mixed\n",
    "5. SDP: Sharded Data-Parallel \n",
    "6. FSDP: Fully Sharded Data-Parallel\n",
    "\n",
    "Best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd162b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "This tutorial covers several plugins for multi-GPU and distributed training and will walk you through the differences between these, when to use one over the other, and best practices in writing hardware- and plugin-agnostic code.\n",
    "We will make use of the following terminology:\n",
    "\n",
    "- **Accelerator:** The hardware type we are running on. This can be a CPU, GPU, TPU or IPU. This tutorial will focus only on the GPU accelerator because it is compatible with a large selection of different plugins.\n",
    "- **Plugin:** Also referred to as \"training type plugin\", \"backend\" or \"distributed backend\" sometimes. A plugin determines how model and data are split across multiple devices and defines the communication and synchronization between devices and processes.\n",
    "\n",
    "**IMPORTANT NOTE:** This notebook is not meant to be executed in full. Some cells will produce an output but many of the backends presented here will NOT run in a Jupyter environment.\n",
    "\n",
    "Throughout the next sections, we will re-use the following templates for the model and data module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1da403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "\n",
    "\n",
    "class MNISTDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir: str = \"./\", batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(), transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ])\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # only downloads the data once\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "    \n",
    "    \n",
    "class TutorialModule(LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 128,\n",
    "        learning_rate: float = 0.0001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\n",
    "        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\n",
    "        self.val_accuracy = Accuracy(num_classes=10)\n",
    "        self.test_accuracy = Accuracy(num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return {\"loss\": loss, \"y_hat\": y_hat.detach()}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        self.log(\"val_acc\", self.val_accuracy(pred, y), prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        self.log(\"test_acc\", self.test_accuracy(pred, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc93cd7",
   "metadata": {},
   "source": [
    "We perform a quick test run to check that the template code works. It should achieve a test accuracy of ~92% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41173666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/trainer.py:1200: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/core/datamodule.py:167: LightningDeprecationWarning: DataModule property `has_prepared_data` was deprecated in v1.4 and will be removed in v1.6.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | l1            | Linear   | 100 K \n",
      "1 | l2            | Linear   | 1.3 K \n",
      "2 | val_accuracy  | Accuracy | 0     \n",
      "3 | test_accuracy | Accuracy | 0     \n",
      "-------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fadda3a139c4177b543ad7e059c0f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605861f6ea3341a7ba4aa626925fb522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.926800012588501}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.926800012588501}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "model = TutorialModule()\n",
    "datamodule = MNISTDataModule()\n",
    "trainer = Trainer(max_epochs=1)\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "trainer.test(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553be908",
   "metadata": {},
   "source": [
    "## Overview of Multi-device Plugins in Lightning\n",
    "\n",
    "Lightning supports a variety of different plugins for multi-GPU/distributed training (we sometimes call these training type plugins, or distributed backend). A plugin determines how model and data are split across multiple devices and defines the communication and synchronization between devices and processes. Below we list all plugins, each with recommendations when to use and when not to use, an example code, and important details to consider for writing device-agnostic and performant code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4db31b",
   "metadata": {},
   "source": [
    "### DP: DataParallel\n",
    "\n",
    "**Use when:** \n",
    "- you want to port an existing PyTorch model written with DataParallel and want to maintain 100% parity;\n",
    "- your optimization needs the full aggregated batch of outputs/losses from all GPUs;\n",
    "- none of the other backends presented here are suitable due to their hardware and runtime requirements.\n",
    "\n",
    "**Do not use when**\n",
    "- you are looking for the most performant multi-GPU code.\n",
    "- you have custom batch structures that can not be converted to primitive containers like tuples, lists, dicts etc.\n",
    "- you rely heavily on torchmetrics\n",
    "\n",
    "**How to activate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cf5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# data-parallel with 2 GPUs\n",
    "trainer = Trainer(gpus=2, accelerator=\"dp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20647836",
   "metadata": {},
   "source": [
    "Data-Parallel initially moves all model parameters, buffers and data tensors to the root GPU. In Lighting, this is GPU 0. The following steps take place in _every_ training step:\n",
    "1. The model gets replicated to every device, i.e., parameters and buffers get copied from the root device to all other devices. \n",
    "2. The data batch that initially resides on GPU 0 gets split into N sub-batches along dimension 0 (batch dimension). Each GPU receives one of these batch splits and they are passed to the ``training_step`` hook.\n",
    "3. The output of ``training_step`` in each device will be transferred back to the root device and averaged.\n",
    "\n",
    "The fact that the module is replicated every forward and backward pass makes this the least efficient plugin for multi-GPU training. An additional caveat is that state changes on the module during ``training_step`` are lost, and this is a common source of bugs. It is also the reason why torchmetrics is not recommended together with this plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa437de",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d17ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPModule(TutorialModule):\n",
    "    \n",
    "    # *_step() happens on the replica of the model (each GPU runs this)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # total batch size = 16, 2 GPUs -> each GPU sees batch of size 8\n",
    "        # the last batch may still be smaller, the dataset may not be evenly divisible by the batch size \n",
    "        assert x.shape[0] <= 8 \n",
    "        prob = F.softmax(self(x), dim=1)\n",
    "        pred = torch.argmax(prob, dim=1)\n",
    "        return pred, y\n",
    "\n",
    "    def validation_step_end(self, outputs):\n",
    "        # torchmetrics do not support sync on the replica\n",
    "        # all torchmetric computations need to be performed in *_step_end()\n",
    "        # which happens on the root device\n",
    "        pred, y = outputs\n",
    "        self.log(\"val_acc\", self.val_accuracy(pred, y), prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26f01bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/core/datamodule.py:167: LightningDeprecationWarning: DataModule property `has_prepared_data` was deprecated in v1.4 and will be removed in v1.6.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | l1            | Linear   | 100 K \n",
      "1 | l2            | Linear   | 1.3 K \n",
      "2 | val_accuracy  | Accuracy | 0     \n",
      "3 | test_accuracy | Accuracy | 0     \n",
      "-------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 1\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f80d0a9b2ce4328bafc335ee72199d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "model = DPModule()\n",
    "\n",
    "datamodule = MNISTDataModule()\n",
    "trainer = Trainer(gpus=2, accelerator=\"dp\", max_epochs=1)\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978189ff",
   "metadata": {},
   "source": [
    "**Example 2: Custom reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "300e3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPModule(TutorialModule):\n",
    "    \n",
    "    def training_step_end(self, outputs):\n",
    "        # outputs is a dict\n",
    "        # it is the result of merging all dicts returned by training_step() on each device\n",
    "        \n",
    "        # the loss from each GPU, 2 GPUs are used here\n",
    "        losses = outputs[\"loss\"]\n",
    "        assert losses.shape[0] == 2\n",
    "        \n",
    "        # each GPU returned 8 predictions\n",
    "        y_hats = outputs[\"y_hat\"]\n",
    "        assert y_hats.shape[0] == 2 * 8\n",
    "        \n",
    "        probs = F.softmax(y_hats, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        loss = torch.mean(losses)\n",
    "        return {\"loss\": loss, \"pred\": preds}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        # we can receive all outputs from all training steps and concatenate them\n",
    "        all_predictions = torch.cat([out[\"pred\"] for out in outputs])\n",
    "        print(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b724fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/core/datamodule.py:167: LightningDeprecationWarning: DataModule property `has_prepared_data` was deprecated in v1.4 and will be removed in v1.6.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/adrian/repositories/pytorch-lightning/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18f5b3033774805b8f64dbdb283e1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 2, 2, 3, 5, 2, 7, 7, 5, 9, 2, 2, 1, 7, 2, 3, 2, 2, 2, 3, 5, 2, 7,\n",
      "        7, 5, 9, 2, 2, 1, 7, 2, 3, 2, 2, 2, 3, 5, 2, 7, 7, 5, 9, 2, 2, 1, 7, 2,\n",
      "        3, 2, 2, 2, 3, 5, 2, 7, 7, 5, 9, 2, 2, 1, 7, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = DPModule()\n",
    "datamodule = MNISTDataModule()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=2, \n",
    "    accelerator=\"dp\", \n",
    "    max_steps=4,\n",
    "    weights_summary=None,\n",
    "    limit_val_batches=0,\n",
    ")\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ceeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f5767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd204ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe970571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0aa84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7ee83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
